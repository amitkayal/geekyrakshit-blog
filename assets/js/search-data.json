{
  
    
        "post0": {
            "title": "Balloon Detection using Mask RCNN",
            "content": "!pip install -U torch torchvision !pip install git+https://github.com/facebookresearch/fvcore.git !git clone https://github.com/facebookresearch/detectron2 detectron2_repo !pip install -e detectron2_repo . Collecting torch Downloading https://files.pythonhosted.org/packages/ae/05/50a05de5337f7a924bb8bd70c6936230642233e424d6a9747ef1cfbde353/torch-1.3.0-cp36-cp36m-manylinux1_x86_64.whl (773.1MB) |████████████████████████████████| 773.1MB 31kB/s Collecting torchvision Downloading https://files.pythonhosted.org/packages/fc/23/d418c9102d4054d19d57ccf0aca18b7c1c1f34cc0a136760b493f78ddb06/torchvision-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (10.1MB) |████████████████████████████████| 10.1MB 27.4MB/s Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.16.5) Requirement already satisfied, skipping upgrade: pillow&gt;=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0) Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0) Requirement already satisfied, skipping upgrade: olefile in /usr/local/lib/python3.6/dist-packages (from pillow&gt;=4.1.1-&gt;torchvision) (0.46) Installing collected packages: torch, torchvision Found existing installation: torch 1.2.0 Uninstalling torch-1.2.0: Successfully uninstalled torch-1.2.0 Found existing installation: torchvision 0.4.0 Uninstalling torchvision-0.4.0: Successfully uninstalled torchvision-0.4.0 Successfully installed torch-1.3.0 torchvision-0.4.1 Collecting git+https://github.com/facebookresearch/fvcore.git Cloning https://github.com/facebookresearch/fvcore.git to /tmp/pip-req-build-w1yn64cw Running command git clone -q https://github.com/facebookresearch/fvcore.git /tmp/pip-req-build-w1yn64cw Collecting yacs&gt;=0.1.6 (from fvcore==0.1) Downloading https://files.pythonhosted.org/packages/2f/51/9d613d67a8561a0cdf696c3909870f157ed85617fea3cff769bb7de09ef7/yacs-0.1.6-py3-none-any.whl Collecting pyyaml&gt;=5.1 (from fvcore==0.1) Downloading https://files.pythonhosted.org/packages/e3/e8/b3212641ee2718d556df0f23f78de8303f068fe29cdaa7a91018849582fe/PyYAML-5.1.2.tar.gz (265kB) |████████████████████████████████| 266kB 9.6MB/s Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1) (4.28.1) Collecting portalocker (from fvcore==0.1) Downloading https://files.pythonhosted.org/packages/60/ec/836a494dbaa72541f691ec4e66f29fdc8db9bcc7f49e1c2d457ba13ced42/portalocker-1.5.1-py2.py3-none-any.whl Requirement already satisfied: termcolor&gt;=1.1 in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1) (1.1.0) Requirement already satisfied: shapely in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1) (1.6.4.post2) Building wheels for collected packages: fvcore, pyyaml Building wheel for fvcore (setup.py) ... done Created wheel for fvcore: filename=fvcore-0.1-cp36-none-any.whl size=30803 sha256=0fe3120a28cd0125e6d8f804af89611178f206dc5b19e4bee4cef5fa9f952568 Stored in directory: /tmp/pip-ephem-wheel-cache-lgmu2lb2/wheels/48/53/79/3c6485543a4455a0006f5db590ab9957622b6227011941de06 Building wheel for pyyaml (setup.py) ... done Created wheel for pyyaml: filename=PyYAML-5.1.2-cp36-cp36m-linux_x86_64.whl size=44104 sha256=ae2b8f96bdf9abf5d40771722b1eaacb18ed7ce5afe06bdd4ecb9075be1d03ec Stored in directory: /root/.cache/pip/wheels/d9/45/dd/65f0b38450c47cf7e5312883deb97d065e030c5cca0a365030 Successfully built fvcore pyyaml Installing collected packages: pyyaml, yacs, portalocker, fvcore Found existing installation: PyYAML 3.13 Uninstalling PyYAML-3.13: Successfully uninstalled PyYAML-3.13 Successfully installed fvcore-0.1 portalocker-1.5.1 pyyaml-5.1.2 yacs-0.1.6 Cloning into &#39;detectron2_repo&#39;... remote: Enumerating objects: 458, done. remote: Counting objects: 100% (458/458), done. remote: Compressing objects: 100% (356/356), done. remote: Total 458 (delta 111), reused 443 (delta 99), pack-reused 0 Receiving objects: 100% (458/458), 1.40 MiB | 3.07 MiB/s, done. Resolving deltas: 100% (111/111), done. Obtaining file:///content/detectron2_repo Requirement already satisfied: termcolor&gt;=1.1 in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1) (1.1.0) Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1) (4.3.0) Requirement already satisfied: yacs&gt;=0.1.6 in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1) (0.1.6) Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1) (0.8.5) Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1) (0.6.1) Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1) (3.0.3) Collecting tqdm&gt;4.29.0 (from detectron2==0.1) Downloading https://files.pythonhosted.org/packages/e1/c1/bc1dba38b48f4ae3c4428aea669c5e27bd5a7642a74c8348451e0bd8ff86/tqdm-4.36.1-py2.py3-none-any.whl (52kB) |████████████████████████████████| 61kB 5.2MB/s Requirement already satisfied: shapely in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1) (1.6.4.post2) Requirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1) (1.15.0) Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow-&gt;detectron2==0.1) (0.46) Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from yacs&gt;=0.1.6-&gt;detectron2==0.1) (5.1.2) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;detectron2==0.1) (0.10.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;detectron2==0.1) (2.4.2) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;detectron2==0.1) (1.1.0) Requirement already satisfied: numpy&gt;=1.10.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;detectron2==0.1) (1.16.5) Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;detectron2==0.1) (2.5.3) Requirement already satisfied: werkzeug&gt;=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard-&gt;detectron2==0.1) (0.16.0) Requirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard-&gt;detectron2==0.1) (3.1.1) Requirement already satisfied: setuptools&gt;=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard-&gt;detectron2==0.1) (41.2.0) Requirement already satisfied: six&gt;=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard-&gt;detectron2==0.1) (1.12.0) Requirement already satisfied: protobuf&gt;=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard-&gt;detectron2==0.1) (3.7.1) Requirement already satisfied: wheel&gt;=0.26; python_version &gt;= &#34;3&#34; in /usr/local/lib/python3.6/dist-packages (from tensorboard-&gt;detectron2==0.1) (0.33.6) Requirement already satisfied: grpcio&gt;=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard-&gt;detectron2==0.1) (1.15.0) Requirement already satisfied: absl-py&gt;=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard-&gt;detectron2==0.1) (0.8.0) Installing collected packages: tqdm, detectron2 Found existing installation: tqdm 4.28.1 Uninstalling tqdm-4.28.1: Successfully uninstalled tqdm-4.28.1 Running setup.py develop for detectron2 Successfully installed detectron2 tqdm-4.36.1 . import detectron2, cv2, random import os, json, itertools import numpy as np import torch, torchvision from detectron2.utils.logger import setup_logger from detectron2.engine import DefaultPredictor from detectron2.config import get_cfg from detectron2.utils.visualizer import Visualizer from detectron2.data import MetadataCatalog from detectron2.structures import BoxMode from detectron2.data import DatasetCatalog, MetadataCatalog from detectron2.engine import DefaultTrainer from detectron2.config import get_cfg from matplotlib import pyplot as plt from google.colab.patches import cv2_imshow setup_logger() . &lt;Logger detectron2 (DEBUG)&gt; . Inference With Pre-Trained Model . !wget http://images.cocodataset.org/val2017/000000439715.jpg -O input.jpg im = cv2.imread(&quot;./input.jpg&quot;) cv2_imshow(im) . --2019-10-11 09:46:46-- http://images.cocodataset.org/val2017/000000439715.jpg Resolving images.cocodataset.org (images.cocodataset.org)... 52.216.106.116 Connecting to images.cocodataset.org (images.cocodataset.org)|52.216.106.116|:80... connected. HTTP request sent, awaiting response... 200 OK Length: 209222 (204K) [image/jpeg] Saving to: ‘input.jpg’ input.jpg 100%[===================&gt;] 204.32K 847KB/s in 0.2s 2019-10-11 09:46:47 (847 KB/s) - ‘input.jpg’ saved [209222/209222] . cfg = get_cfg() # get model from https://github.com/facebookresearch/detectron2/tree/master/configs cfg.merge_from_file(&quot;./detectron2_repo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml&quot;) cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5 # set threshold for this model # Weights ke pkl kre -_- cfg.MODEL.WEIGHTS = &quot;detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl&quot; predictor = DefaultPredictor(cfg) outputs = predictor(im) . WARNING [10/11 09:46:54 d2.config.compat]: Config &#39;./detectron2_repo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml&#39; has no VERSION. Assuming it to be compatible with latest v2. . # look at the outputs print(&#39;Classes of instances:&#39;, outputs[&quot;instances&quot;].pred_classes) print(&#39;Bboxes of instances:&#39;, outputs[&quot;instances&quot;].pred_boxes) . Classes of instances: tensor([17, 0, 0, 0, 0, 0, 0, 0, 25, 0, 25, 25, 0, 0, 24], device=&#39;cuda:0&#39;) Bboxes of instances: Boxes(tensor([[126.6035, 244.8977, 459.8291, 480.0000], [251.1083, 157.8127, 338.9731, 413.6379], [114.8496, 268.6864, 148.2352, 398.8111], [ 0.8217, 281.0327, 78.6073, 478.4210], [ 49.3954, 274.1229, 80.1545, 342.9808], [561.2248, 271.5816, 596.2755, 385.2552], [385.9072, 270.3125, 413.7130, 304.0397], [515.9295, 278.3744, 562.2792, 389.3802], [335.2409, 251.9167, 414.7491, 275.9375], [350.9300, 269.2060, 386.0984, 297.9081], [331.6292, 230.9996, 393.2759, 257.2009], [510.7349, 263.2656, 570.9865, 295.9194], [409.0841, 271.8646, 460.5582, 356.8722], [506.8766, 283.3257, 529.9404, 324.0392], [594.5663, 283.4820, 609.0577, 311.4124]], device=&#39;cuda:0&#39;)) . # They also have visualization utils :P v = Visualizer(im, MetadataCatalog.get(&quot;coco_2017_val&quot;), scale = 1.5) v = v.draw_instance_predictions(outputs[&quot;instances&quot;].to(&quot;cpu&quot;)) cv2_imshow(v.get_image()[:, :, ::-1]) . Training on Custom Dataset . !wget https://github.com/matterport/Mask_RCNN/releases/download/v2.1/balloon_dataset.zip !unzip balloon_dataset.zip &gt; /dev/null . --2019-10-11 09:47:11-- https://github.com/matterport/Mask_RCNN/releases/download/v2.1/balloon_dataset.zip Resolving github.com (github.com)... 140.82.118.3 Connecting to github.com (github.com)|140.82.118.3|:443... connected. HTTP request sent, awaiting response... 302 Found Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/107595270/737339e2-2b83-11e8-856a-188034eb3468?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191011%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20191011T094712Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=7d6e77a1f62604e8eeea14871837711e05770f7ccefe7876722acc45d79590c6&amp;X-Amz-SignedHeaders=host&amp;actor_id=0&amp;response-content-disposition=attachment%3B%20filename%3Dballoon_dataset.zip&amp;response-content-type=application%2Foctet-stream [following] --2019-10-11 09:47:13-- https://github-production-release-asset-2e65be.s3.amazonaws.com/107595270/737339e2-2b83-11e8-856a-188034eb3468?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191011%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20191011T094712Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=7d6e77a1f62604e8eeea14871837711e05770f7ccefe7876722acc45d79590c6&amp;X-Amz-SignedHeaders=host&amp;actor_id=0&amp;response-content-disposition=attachment%3B%20filename%3Dballoon_dataset.zip&amp;response-content-type=application%2Foctet-stream Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.165.179 Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.165.179|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 38741381 (37M) [application/octet-stream] Saving to: ‘balloon_dataset.zip.1’ balloon_dataset.zip 100%[===================&gt;] 36.95M 23.9MB/s in 1.5s 2019-10-11 09:47:15 (23.9 MB/s) - ‘balloon_dataset.zip.1’ saved [38741381/38741381] replace balloon/train/via_region_data.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: n replace __MACOSX/balloon/train/._via_region_data.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: n replace balloon/train/53500107_d24b11b3c2_b.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: n replace __MACOSX/balloon/train/._53500107_d24b11b3c2_b.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: . def get_balloon_dicts(img_dir): json_file = os.path.join(img_dir, &quot;via_region_data.json&quot;) with open(json_file) as f: imgs_anns = json.load(f) dataset_dicts = [] for _, v in imgs_anns.items(): record = {} filename = os.path.join(img_dir, v[&quot;filename&quot;]) height, width = cv2.imread(filename).shape[:2] record[&quot;file_name&quot;] = filename record[&quot;height&quot;] = height record[&quot;width&quot;] = width annos = v[&quot;regions&quot;] objs = [] for _, anno in annos.items(): assert not anno[&quot;region_attributes&quot;] anno = anno[&quot;shape_attributes&quot;] px = anno[&quot;all_points_x&quot;] py = anno[&quot;all_points_y&quot;] poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)] poly = list(itertools.chain.from_iterable(poly)) obj = { &quot;bbox&quot;: [np.min(px), np.min(py), np.max(px), np.max(py)], &quot;bbox_mode&quot;: BoxMode.XYXY_ABS, &quot;segmentation&quot;: [poly], &quot;category_id&quot;: 0, &quot;iscrowd&quot;: 0 } objs.append(obj) record[&quot;annotations&quot;] = objs dataset_dicts.append(record) return dataset_dicts . #hide_output get_balloon_dicts(&quot;./balloon/train&quot;) . # Register Balloon Dataset for d in [&quot;train&quot;, &quot;val&quot;]: DatasetCatalog.register(&quot;balloon/&quot; + d, lambda d=d: get_balloon_dicts(&quot;balloon/&quot; + d)) MetadataCatalog.get(&quot;balloon/&quot; + d).set(thing_classes=[&quot;balloon&quot;]) balloon_metadata = MetadataCatalog.get(&quot;balloon/train&quot;) . # Visualize Dataset dataset_dicts = get_balloon_dicts(&quot;balloon/train&quot;) for d in random.sample(dataset_dicts, 3): img = cv2.imread(d[&quot;file_name&quot;]) visualizer = Visualizer(img[:, :, ::-1], metadata=balloon_metadata, scale=0.5) vis = visualizer.draw_dataset_dict(d) cv2_imshow(vis.get_image()[:, :, ::-1]) . # Set the training Configs cfg = get_cfg() cfg.merge_from_file(&quot;./detectron2_repo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml&quot;) cfg.DATASETS.TRAIN = (&quot;balloon/train&quot;,) cfg.DATASETS.TEST = () # no metrics implemented for this dataset cfg.DATALOADER.NUM_WORKERS = 2 cfg.MODEL.WEIGHTS = &quot;detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl&quot; # initialize from model zoo cfg.SOLVER.IMS_PER_BATCH = 2 cfg.SOLVER.BASE_LR = 0.00025 cfg.SOLVER.MAX_ITER = 300 # 300 iterations seems good enough, but you can certainly train longer cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128 # faster, and good enough for this toy dataset cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1 # only has one class (ballon) . WARNING [10/11 09:48:44 d2.config.compat]: Config &#39;./detectron2_repo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml&#39; has no VERSION. Assuming it to be compatible with latest v2. . # Trainer os.makedirs(cfg.OUTPUT_DIR, exist_ok = True) trainer = DefaultTrainer(cfg) trainer.resume_or_load(resume = False) trainer.train() . [10/11 09:48:55 d2.engine.defaults]: Model: GeneralizedRCNN( (backbone): FPN( (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1)) (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1)) (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1)) (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1)) (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (top_block): LastLevelMaxPool() (bottom_up): ResNet( (stem): BasicStem( (conv1): Conv2d( 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05) ) ) (res2): Sequential( (0): BottleneckBlock( (shortcut): Conv2d( 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05) ) (conv1): Conv2d( 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05) ) (conv2): Conv2d( 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05) ) (conv3): Conv2d( 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05) ) ) (1): BottleneckBlock( (conv1): Conv2d( 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05) ) (conv2): Conv2d( 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05) ) (conv3): Conv2d( 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05) ) ) (2): BottleneckBlock( (conv1): Conv2d( 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05) ) (conv2): Conv2d( 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05) ) (conv3): Conv2d( 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05) ) ) ) (res3): Sequential( (0): BottleneckBlock( (shortcut): Conv2d( 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05) ) (conv1): Conv2d( 256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05) ) (conv2): Conv2d( 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05) ) (conv3): Conv2d( 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05) ) ) (1): BottleneckBlock( (conv1): Conv2d( 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05) ) (conv2): Conv2d( 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05) ) (conv3): Conv2d( 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05) ) ) (2): BottleneckBlock( (conv1): Conv2d( 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05) ) (conv2): Conv2d( 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05) ) (conv3): Conv2d( 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05) ) ) (3): BottleneckBlock( (conv1): Conv2d( 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05) ) (conv2): Conv2d( 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05) ) (conv3): Conv2d( 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05) ) ) ) (res4): Sequential( (0): BottleneckBlock( (shortcut): Conv2d( 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05) ) (conv1): Conv2d( 512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05) ) (conv2): Conv2d( 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05) ) (conv3): Conv2d( 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05) ) ) (1): BottleneckBlock( (conv1): Conv2d( 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05) ) (conv2): Conv2d( 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05) ) (conv3): Conv2d( 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05) ) ) (2): BottleneckBlock( (conv1): Conv2d( 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05) ) (conv2): Conv2d( 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05) ) (conv3): Conv2d( 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05) ) ) (3): BottleneckBlock( (conv1): Conv2d( 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05) ) (conv2): Conv2d( 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05) ) (conv3): Conv2d( 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05) ) ) (4): BottleneckBlock( (conv1): Conv2d( 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05) ) (conv2): Conv2d( 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05) ) (conv3): Conv2d( 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05) ) ) (5): BottleneckBlock( (conv1): Conv2d( 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05) ) (conv2): Conv2d( 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05) ) (conv3): Conv2d( 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05) ) ) ) (res5): Sequential( (0): BottleneckBlock( (shortcut): Conv2d( 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05) ) (conv1): Conv2d( 1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05) ) (conv2): Conv2d( 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05) ) (conv3): Conv2d( 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05) ) ) (1): BottleneckBlock( (conv1): Conv2d( 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05) ) (conv2): Conv2d( 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05) ) (conv3): Conv2d( 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05) ) ) (2): BottleneckBlock( (conv1): Conv2d( 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05) ) (conv2): Conv2d( 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05) ) (conv3): Conv2d( 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05) ) ) ) ) ) (proposal_generator): RPN( (anchor_generator): DefaultAnchorGenerator( (cell_anchors): BufferList() ) (rpn_head): StandardRPNHead( (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1)) (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1)) ) ) (roi_heads): StandardROIHeads( (box_pooler): ROIPooler( (level_poolers): ModuleList( (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True) (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True) (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True) (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True) ) ) (box_head): FastRCNNConvFCHead( (fc1): Linear(in_features=12544, out_features=1024, bias=True) (fc2): Linear(in_features=1024, out_features=1024, bias=True) ) (box_predictor): FastRCNNOutputLayers( (cls_score): Linear(in_features=1024, out_features=2, bias=True) (bbox_pred): Linear(in_features=1024, out_features=4, bias=True) ) (mask_pooler): ROIPooler( (level_poolers): ModuleList( (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True) (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True) (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True) (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True) ) ) (mask_head): MaskRCNNConvUpsampleHead( (mask_fcn1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (mask_fcn2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (mask_fcn3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (mask_fcn4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2)) (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1)) ) ) ) [10/11 09:48:57 d2.data.build]: Removed 0 images with no usable annotations. 61 images left. [10/11 09:48:57 d2.data.build]: Distribution of training instances among all 1 categories: | category | #instances | |:-:|:-| | balloon | 255 | | | | [10/11 09:48:57 d2.data.detection_utils]: TransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style=&#39;choice&#39;), RandomFlip()] [10/11 09:48:57 d2.data.build]: Using training sampler TrainingSampler . &#39;roi_heads.box_predictor.cls_score.weight&#39; has shape (81, 1024) in the checkpoint but (2, 1024) in the model! Skipped. &#39;roi_heads.box_predictor.cls_score.bias&#39; has shape (81,) in the checkpoint but (2,) in the model! Skipped. &#39;roi_heads.box_predictor.bbox_pred.weight&#39; has shape (320, 1024) in the checkpoint but (4, 1024) in the model! Skipped. &#39;roi_heads.box_predictor.bbox_pred.bias&#39; has shape (320,) in the checkpoint but (4,) in the model! Skipped. &#39;roi_heads.mask_head.predictor.weight&#39; has shape (80, 256, 1, 1) in the checkpoint but (1, 256, 1, 1) in the model! Skipped. &#39;roi_heads.mask_head.predictor.bias&#39; has shape (80,) in the checkpoint but (1,) in the model! Skipped. . [10/11 09:49:01 d2.engine.train_loop]: Starting training from iteration 0 [10/11 09:49:25 d2.utils.events]: eta: 0:05:35 iter: 19 total_loss: 2.006 loss_cls: 0.685 loss_box_reg: 0.608 loss_mask: 0.692 loss_rpn_cls: 0.021 loss_rpn_loc: 0.005 time: 1.2004 data_time: 0.0035 lr: 0.000005 max_mem: 2725M [10/11 09:49:49 d2.utils.events]: eta: 0:05:07 iter: 39 total_loss: 2.141 loss_cls: 0.650 loss_box_reg: 0.658 loss_mask: 0.663 loss_rpn_cls: 0.040 loss_rpn_loc: 0.010 time: 1.2014 data_time: 0.0038 lr: 0.000010 max_mem: 2726M [10/11 09:50:14 d2.utils.events]: eta: 0:04:53 iter: 59 total_loss: 1.734 loss_cls: 0.581 loss_box_reg: 0.583 loss_mask: 0.603 loss_rpn_cls: 0.018 loss_rpn_loc: 0.005 time: 1.2228 data_time: 0.0035 lr: 0.000015 max_mem: 2850M [10/11 09:50:38 d2.utils.events]: eta: 0:04:26 iter: 79 total_loss: 1.721 loss_cls: 0.509 loss_box_reg: 0.633 loss_mask: 0.538 loss_rpn_cls: 0.033 loss_rpn_loc: 0.007 time: 1.2148 data_time: 0.0039 lr: 0.000020 max_mem: 2850M [10/11 09:51:03 d2.utils.events]: eta: 0:04:04 iter: 99 total_loss: 1.681 loss_cls: 0.442 loss_box_reg: 0.700 loss_mask: 0.479 loss_rpn_cls: 0.031 loss_rpn_loc: 0.008 time: 1.2220 data_time: 0.0036 lr: 0.000025 max_mem: 2850M [10/11 09:51:29 d2.utils.events]: eta: 0:03:42 iter: 119 total_loss: 1.519 loss_cls: 0.408 loss_box_reg: 0.611 loss_mask: 0.401 loss_rpn_cls: 0.028 loss_rpn_loc: 0.004 time: 1.2303 data_time: 0.0036 lr: 0.000030 max_mem: 2850M [10/11 09:51:54 d2.utils.events]: eta: 0:03:18 iter: 139 total_loss: 1.490 loss_cls: 0.370 loss_box_reg: 0.696 loss_mask: 0.364 loss_rpn_cls: 0.018 loss_rpn_loc: 0.011 time: 1.2313 data_time: 0.0036 lr: 0.000035 max_mem: 2850M [10/11 09:52:18 d2.utils.events]: eta: 0:02:53 iter: 159 total_loss: 1.293 loss_cls: 0.307 loss_box_reg: 0.611 loss_mask: 0.303 loss_rpn_cls: 0.021 loss_rpn_loc: 0.006 time: 1.2321 data_time: 0.0034 lr: 0.000040 max_mem: 2850M [10/11 09:52:43 d2.utils.events]: eta: 0:02:30 iter: 179 total_loss: 1.221 loss_cls: 0.279 loss_box_reg: 0.629 loss_mask: 0.287 loss_rpn_cls: 0.024 loss_rpn_loc: 0.011 time: 1.2337 data_time: 0.0035 lr: 0.000045 max_mem: 2850M [10/11 09:53:08 d2.utils.events]: eta: 0:02:05 iter: 199 total_loss: 1.203 loss_cls: 0.242 loss_box_reg: 0.639 loss_mask: 0.241 loss_rpn_cls: 0.020 loss_rpn_loc: 0.006 time: 1.2365 data_time: 0.0035 lr: 0.000050 max_mem: 2850M [10/11 09:53:34 d2.utils.events]: eta: 0:01:40 iter: 219 total_loss: 1.109 loss_cls: 0.234 loss_box_reg: 0.659 loss_mask: 0.205 loss_rpn_cls: 0.015 loss_rpn_loc: 0.011 time: 1.2379 data_time: 0.0035 lr: 0.000055 max_mem: 2850M [10/11 09:53:59 d2.utils.events]: eta: 0:01:16 iter: 239 total_loss: 1.023 loss_cls: 0.202 loss_box_reg: 0.596 loss_mask: 0.186 loss_rpn_cls: 0.021 loss_rpn_loc: 0.009 time: 1.2428 data_time: 0.0034 lr: 0.000060 max_mem: 2850M [10/11 09:54:24 d2.utils.events]: eta: 0:00:51 iter: 259 total_loss: 0.989 loss_cls: 0.192 loss_box_reg: 0.599 loss_mask: 0.179 loss_rpn_cls: 0.020 loss_rpn_loc: 0.007 time: 1.2424 data_time: 0.0035 lr: 0.000065 max_mem: 2850M [10/11 09:54:49 d2.utils.events]: eta: 0:00:26 iter: 279 total_loss: 0.944 loss_cls: 0.155 loss_box_reg: 0.564 loss_mask: 0.151 loss_rpn_cls: 0.026 loss_rpn_loc: 0.011 time: 1.2431 data_time: 0.0034 lr: 0.000070 max_mem: 2850M [10/11 09:55:17 d2.utils.events]: eta: 0:00:01 iter: 299 total_loss: 0.761 loss_cls: 0.135 loss_box_reg: 0.468 loss_mask: 0.119 loss_rpn_cls: 0.017 loss_rpn_loc: 0.006 time: 1.2489 data_time: 0.0035 lr: 0.000075 max_mem: 2850M [10/11 09:55:18 d2.engine.hooks]: Overall training speed: 297 iterations in 0:06:12 (1.2531 s / it) [10/11 09:55:18 d2.engine.hooks]: Total training time: 0:06:14 (0:00:02 on hooks) . OrderedDict() . # Save model for testing cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, &quot;model_final.pth&quot;) cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7 # set the testing threshold for this model cfg.DATASETS.TEST = (&quot;balloon_val&quot;, ) predictor = DefaultPredictor(cfg) . # Inference on Validation Set from detectron2.utils.visualizer import ColorMode dataset_dicts = get_balloon_dicts(&quot;balloon/val&quot;) for d in random.sample(dataset_dicts, 3): im = cv2.imread(d[&quot;file_name&quot;]) outputs = predictor(im) v = Visualizer(im[:, :, ::-1], metadata=MetadataCatalog.get(&quot;balloon_val&quot;), scale=0.8, instance_mode=ColorMode.IMAGE_BW # remove the colors of unsegmented pixels ) v = v.draw_instance_predictions(outputs[&quot;instances&quot;].to(&quot;cpu&quot;)) cv2_imshow(v.get_image()[:, :, ::-1]) .",
            "url": "https://soumik12345.github.io/geekyrakshit-blog/computervision/deeplearning/segmentation/objectdetction/neuralnetwork/instancesegmentation/convolution/detectron/maskrcnn/python/pytorch/2020/04/13/detectron-mask-rcnn.html",
            "relUrl": "/computervision/deeplearning/segmentation/objectdetction/neuralnetwork/instancesegmentation/convolution/detectron/maskrcnn/python/pytorch/2020/04/13/detectron-mask-rcnn.html",
            "date": " • Apr 13, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Automatic Number Plate Recognition",
            "content": "Generating Training Data . #collapse-hide import os print(&#39;Generating Training Data n&#39;) os.chdir(&#39;./Korean-license-plate-Generator&#39;) !mkdir ../Train_Images print(&#39;Augmentated Images Generating... n&#39;) !python ./Generator_augmentation.py --img_dir &quot;../Train_Images/&quot; --num 10000 --save True print(&#39;Augmentated Images Generated n&#39;) print(&#39;Original Images Generating... n&#39;) !python ./Generator_original.py --img_dir &quot;../Train_Images/&quot; --num 10000 --save True print(&#39;Original Images Generated n&#39;) print(&#39;Perspective Images Generating... n&#39;) !python ./Generator_perspective.py --img_dir &quot;../Train_Images/&quot; --num 10000 --save True print(&#39;Perspective Images Generated n&#39;) os.chdir(&#39;../&#39;) print(&#39;Done.&#39;) . . Generating Training Data Augmentated Images Generating... Type 1 finish Type 2 finish Type 3 finish Type 4 finish Type 5 finish Augmentated Images Generated Original Images Generating... Type 1 finish Type 2 finish Type 3 finish Type 4 finish Type 5 finish Original Images Generated Perspective Images Generating... Type 1 finish Type 2 finish Type 3 finish Type 4 finish Type 5 finish Perspective Images Generated Done. . Generating Validation Data . #collapse-hide print(&#39;Generating Validation Data n&#39;) os.chdir(&#39;./Korean-license-plate-Generator&#39;) !mkdir ../Validation_Images print(&#39;Augmentated Images Generating... n&#39;) !python ./Generator_augmentation.py --img_dir &quot;../Validation_Images/&quot; --num 1000 --save True print(&#39;Augmentated Images Generated n&#39;) print(&#39;Original Images Generating... n&#39;) !python ./Generator_original.py --img_dir &quot;../Validation_Images/&quot; --num 1000 --save True print(&#39;Original Images Generated n&#39;) print(&#39;Perspective Images Generating... n&#39;) !python ./Generator_perspective.py --img_dir &quot;../Validation_Images/&quot; --num 1000 --save True print(&#39;Perspective Images Generated n&#39;) os.chdir(&#39;../&#39;) print(&#39;Done.&#39;) . . Generating Validation Data Augmentated Images Generating... Type 1 finish Type 2 finish Type 3 finish Type 4 finish Type 5 finish Augmentated Images Generated Original Images Generating... Type 1 finish Type 2 finish Type 3 finish Type 4 finish Type 5 finish Original Images Generated Perspective Images Generating... Type 1 finish Type 2 finish Type 3 finish Type 4 finish Type 5 finish Perspective Images Generated Done. . Importing Libraries . #collapse-hide import cv2 import itertools import os, random import numpy as np from glob import glob from tqdm import tqdm_notebook from matplotlib import pyplot as plt from keras import backend as K from keras.layers import ( Conv2D, MaxPooling2D, Input, Dense, Activation, Reshape, Lambda, BatchNormalization, CuDNNLSTM ) from keras.layers.merge import add, concatenate from keras.models import Model from keras.optimizers import Adadelta from keras.callbacks import EarlyStopping, ModelCheckpoint from keras.utils import plot_model, Sequence . . Using TensorFlow backend. . Configs . CHAR_VECTOR = &quot;adefghjknqrstwABCDEFGHIJKLMNOPZ0123456789&quot; letters = [letter for letter in CHAR_VECTOR] num_classes = len(letters) + 1 img_w, img_h = 128, 64 # Network parameters batch_size = 128 val_batch_size = 16 downsample_factor = 4 max_text_len = 9 K.set_learning_phase(0) . WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead. . Data Generation . def labels_to_text(labels): return &#39;&#39;.join(list(map(lambda x: letters[int(x)], labels))) def text_to_labels(text): return list(map(lambda x: letters.index(x), text)) . class DataGenerator(Sequence): def __init__(self, image_path, batch_size, image_width, image_height, downsample_factor, shuffle = True, max_text_len = 9): &#39;&#39;&#39;Datagenerator&#39;&#39;&#39; self.image_path = image_path self.image_list = glob(self.image_path + &#39;/*&#39;) self.batch_size = batch_size self.image_width = image_width self.image_height = image_height self.downsample_factor = downsample_factor self.shuffle = shuffle self.max_text_len = max_text_len self.on_epoch_end() def __len__(self): &#39;&#39;&#39;Denotes the number of batches per epoch&#39;&#39;&#39; return int(np.floor(len(self.image_list) / self.batch_size)) def __getitem__(self, index): &#39;&#39;&#39;Generate Single Batch of Data&#39;&#39;&#39; # Generate indexes of the batch indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size] # Find list of IDs list_IDs_temp = [self.image_list[k] for k in indexes] # Generate data inputs, outputs = self.__data_generation(list_IDs_temp) return inputs, outputs def on_epoch_end(self): &#39;&#39;&#39;Updates indexes after each epoch&#39;&#39;&#39; self.indexes = np.arange(len(self.image_list)) if self.shuffle == True: np.random.shuffle(self.indexes) def __data_generation(self, list_IDs_temp): &#39;&#39;&#39;Generates data containing batch_size samples&#39;&#39;&#39; # Reading Data images = [] texts = [] label_length = np.zeros((self.batch_size, 1)) for i, img_file in enumerate(list_IDs_temp): image = cv2.imread(img_file, cv2.IMREAD_GRAYSCALE) image = cv2.resize(image, (self.image_width, self.image_height)) image = image.astype(np.float32) image = (image / 255.0) images.append(image.T) text = img_file[0 : -4].split(&#39;/&#39;)[-1] texts.append(text_to_labels(text)) label_length[i] = len(text) # images[i, :, :] = image input_length = np.ones((self.batch_size, 1)) * (self.image_width // self.downsample_factor - 2) images = np.expand_dims(np.array(images), axis = 3) inputs = { &#39;the_input&#39;: images, # (bs, 128, 64, 1) &#39;the_labels&#39;: np.array(texts), # (bs, 8) &#39;input_length&#39;: input_length, # (bs, 1) &#39;label_length&#39;: np.array(label_length) # (bs, 1) } outputs = {&#39;ctc&#39;: np.zeros([self.batch_size])} # (bs, 1) return (inputs, outputs) . datagen = DataGenerator(&#39;./Train_Images&#39;, batch_size, 128, 64, 4, True, 9) . fig, axes = plt.subplots(nrows = 4, ncols = 2, figsize = (16, 16)) plt.setp(axes.flat, xticks = [], yticks = []) inp, out = datagen.__getitem__(0) c = 1 for i, ax in enumerate(axes.flat): ax.imshow(inp[&#39;the_input&#39;][c].reshape(img_w, img_h).T) ax.set_xlabel(labels_to_text(inp[&#39;the_labels&#39;][c])) c += 1 plt.show() . Network Architecture . # # Loss and train functions, network architecture def ctc_lambda_func(args): y_pred, labels, input_length, label_length = args # the 2 is critical here since the first couple outputs of the RNN # tend to be garbage: y_pred = y_pred[:, 2:, :] return K.ctc_batch_cost(labels, y_pred, input_length, label_length) . def get_Model(training): input_shape = (img_w, img_h, 1) # (128, 64, 1) # Make Networkw inputs = Input(name=&#39;the_input&#39;, shape=input_shape, dtype=&#39;float32&#39;) # (None, 128, 64, 1) # Convolution layer (VGG) inner = Conv2D(64, (3, 3), padding=&#39;same&#39;, name=&#39;conv1&#39;, kernel_initializer=&#39;he_normal&#39;)(inputs) # (None, 128, 64, 64) inner = BatchNormalization()(inner) inner = Activation(&#39;relu&#39;)(inner) inner = MaxPooling2D(pool_size=(2, 2), name=&#39;max1&#39;)(inner) # (None,64, 32, 64) inner = Conv2D(128, (3, 3), padding=&#39;same&#39;, name=&#39;conv2&#39;, kernel_initializer=&#39;he_normal&#39;)(inner) # (None, 64, 32, 128) inner = BatchNormalization()(inner) inner = Activation(&#39;relu&#39;)(inner) inner = MaxPooling2D(pool_size=(2, 2), name=&#39;max2&#39;)(inner) # (None, 32, 16, 128) inner = Conv2D(256, (3, 3), padding=&#39;same&#39;, name=&#39;conv3&#39;, kernel_initializer=&#39;he_normal&#39;)(inner) # (None, 32, 16, 256) inner = BatchNormalization()(inner) inner = Activation(&#39;relu&#39;)(inner) inner = Conv2D(256, (3, 3), padding=&#39;same&#39;, name=&#39;conv4&#39;, kernel_initializer=&#39;he_normal&#39;)(inner) # (None, 32, 16, 256) inner = BatchNormalization()(inner) inner = Activation(&#39;relu&#39;)(inner) inner = MaxPooling2D(pool_size=(1, 2), name=&#39;max3&#39;)(inner) # (None, 32, 8, 256) inner = Conv2D(512, (3, 3), padding=&#39;same&#39;, name=&#39;conv5&#39;, kernel_initializer=&#39;he_normal&#39;)(inner) # (None, 32, 8, 512) inner = BatchNormalization()(inner) inner = Activation(&#39;relu&#39;)(inner) inner = Conv2D(512, (3, 3), padding=&#39;same&#39;, name=&#39;conv6&#39;)(inner) # (None, 32, 8, 512) inner = BatchNormalization()(inner) inner = Activation(&#39;relu&#39;)(inner) inner = MaxPooling2D(pool_size=(1, 2), name=&#39;max4&#39;)(inner) # (None, 32, 4, 512) inner = Conv2D(512, (2, 2), padding=&#39;same&#39;, kernel_initializer=&#39;he_normal&#39;, name=&#39;con7&#39;)(inner) # (None, 32, 4, 512) inner = BatchNormalization()(inner) inner = Activation(&#39;relu&#39;)(inner) # CNN to RNN inner = Reshape(target_shape=((32, 2048)), name=&#39;reshape&#39;)(inner) # (None, 32, 2048) inner = Dense(64, activation=&#39;relu&#39;, kernel_initializer=&#39;he_normal&#39;, name=&#39;dense1&#39;)(inner) # (None, 32, 64) # RNN layer lstm_1 = CuDNNLSTM(256, return_sequences=True, kernel_initializer=&#39;he_normal&#39;, name=&#39;lstm1&#39;)(inner) # (None, 32, 512) lstm_1b = CuDNNLSTM(256, return_sequences=True, go_backwards=True, kernel_initializer=&#39;he_normal&#39;, name=&#39;lstm1_b&#39;)(inner) lstm1_merged = add([lstm_1, lstm_1b]) # (None, 32, 512) lstm1_merged = BatchNormalization()(lstm1_merged) lstm_2 = CuDNNLSTM(256, return_sequences=True, kernel_initializer=&#39;he_normal&#39;, name=&#39;lstm2&#39;)(lstm1_merged) lstm_2b = CuDNNLSTM(256, return_sequences=True, go_backwards=True, kernel_initializer=&#39;he_normal&#39;, name=&#39;lstm2_b&#39;)(lstm1_merged) lstm2_merged = concatenate([lstm_2, lstm_2b]) # (None, 32, 1024) lstm_merged = BatchNormalization()(lstm2_merged) # transforms RNN output to character activations: inner = Dense(num_classes, kernel_initializer=&#39;he_normal&#39;,name=&#39;dense2&#39;)(lstm2_merged) #(None, 32, 63) y_pred = Activation(&#39;softmax&#39;, name=&#39;softmax&#39;)(inner) labels = Input(name=&#39;the_labels&#39;, shape=[max_text_len], dtype=&#39;float32&#39;) # (None ,8) input_length = Input(name=&#39;input_length&#39;, shape=[1], dtype=&#39;int64&#39;) # (None, 1) label_length = Input(name=&#39;label_length&#39;, shape=[1], dtype=&#39;int64&#39;) # (None, 1) # Keras doesn&#39;t currently support loss funcs with extra parameters # so CTC loss is implemented in a lambda layer loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name=&#39;ctc&#39;)([y_pred, labels, input_length, label_length]) #(None, 1) if training: return Model(inputs=[inputs, labels, input_length, label_length], outputs=loss_out) else: return Model(inputs=[inputs], outputs=y_pred) . model = get_Model(training = True) model.summary() . WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where Model: &#34;model_1&#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== the_input (InputLayer) (None, 128, 64, 1) 0 __________________________________________________________________________________________________ conv1 (Conv2D) (None, 128, 64, 64) 640 the_input[0][0] __________________________________________________________________________________________________ batch_normalization_1 (BatchNor (None, 128, 64, 64) 256 conv1[0][0] __________________________________________________________________________________________________ activation_1 (Activation) (None, 128, 64, 64) 0 batch_normalization_1[0][0] __________________________________________________________________________________________________ max1 (MaxPooling2D) (None, 64, 32, 64) 0 activation_1[0][0] __________________________________________________________________________________________________ conv2 (Conv2D) (None, 64, 32, 128) 73856 max1[0][0] __________________________________________________________________________________________________ batch_normalization_2 (BatchNor (None, 64, 32, 128) 512 conv2[0][0] __________________________________________________________________________________________________ activation_2 (Activation) (None, 64, 32, 128) 0 batch_normalization_2[0][0] __________________________________________________________________________________________________ max2 (MaxPooling2D) (None, 32, 16, 128) 0 activation_2[0][0] __________________________________________________________________________________________________ conv3 (Conv2D) (None, 32, 16, 256) 295168 max2[0][0] __________________________________________________________________________________________________ batch_normalization_3 (BatchNor (None, 32, 16, 256) 1024 conv3[0][0] __________________________________________________________________________________________________ activation_3 (Activation) (None, 32, 16, 256) 0 batch_normalization_3[0][0] __________________________________________________________________________________________________ conv4 (Conv2D) (None, 32, 16, 256) 590080 activation_3[0][0] __________________________________________________________________________________________________ batch_normalization_4 (BatchNor (None, 32, 16, 256) 1024 conv4[0][0] __________________________________________________________________________________________________ activation_4 (Activation) (None, 32, 16, 256) 0 batch_normalization_4[0][0] __________________________________________________________________________________________________ max3 (MaxPooling2D) (None, 32, 8, 256) 0 activation_4[0][0] __________________________________________________________________________________________________ conv5 (Conv2D) (None, 32, 8, 512) 1180160 max3[0][0] __________________________________________________________________________________________________ batch_normalization_5 (BatchNor (None, 32, 8, 512) 2048 conv5[0][0] __________________________________________________________________________________________________ activation_5 (Activation) (None, 32, 8, 512) 0 batch_normalization_5[0][0] __________________________________________________________________________________________________ conv6 (Conv2D) (None, 32, 8, 512) 2359808 activation_5[0][0] __________________________________________________________________________________________________ batch_normalization_6 (BatchNor (None, 32, 8, 512) 2048 conv6[0][0] __________________________________________________________________________________________________ activation_6 (Activation) (None, 32, 8, 512) 0 batch_normalization_6[0][0] __________________________________________________________________________________________________ max4 (MaxPooling2D) (None, 32, 4, 512) 0 activation_6[0][0] __________________________________________________________________________________________________ con7 (Conv2D) (None, 32, 4, 512) 1049088 max4[0][0] __________________________________________________________________________________________________ batch_normalization_7 (BatchNor (None, 32, 4, 512) 2048 con7[0][0] __________________________________________________________________________________________________ activation_7 (Activation) (None, 32, 4, 512) 0 batch_normalization_7[0][0] __________________________________________________________________________________________________ reshape (Reshape) (None, 32, 2048) 0 activation_7[0][0] __________________________________________________________________________________________________ dense1 (Dense) (None, 32, 64) 131136 reshape[0][0] __________________________________________________________________________________________________ lstm1 (CuDNNLSTM) (None, 32, 256) 329728 dense1[0][0] __________________________________________________________________________________________________ lstm1_b (CuDNNLSTM) (None, 32, 256) 329728 dense1[0][0] __________________________________________________________________________________________________ add_1 (Add) (None, 32, 256) 0 lstm1[0][0] lstm1_b[0][0] __________________________________________________________________________________________________ batch_normalization_8 (BatchNor (None, 32, 256) 1024 add_1[0][0] __________________________________________________________________________________________________ lstm2 (CuDNNLSTM) (None, 32, 256) 526336 batch_normalization_8[0][0] __________________________________________________________________________________________________ lstm2_b (CuDNNLSTM) (None, 32, 256) 526336 batch_normalization_8[0][0] __________________________________________________________________________________________________ concatenate_1 (Concatenate) (None, 32, 512) 0 lstm2[0][0] lstm2_b[0][0] __________________________________________________________________________________________________ dense2 (Dense) (None, 32, 42) 21546 concatenate_1[0][0] __________________________________________________________________________________________________ softmax (Activation) (None, 32, 42) 0 dense2[0][0] __________________________________________________________________________________________________ the_labels (InputLayer) (None, 9) 0 __________________________________________________________________________________________________ input_length (InputLayer) (None, 1) 0 __________________________________________________________________________________________________ label_length (InputLayer) (None, 1) 0 __________________________________________________________________________________________________ ctc (Lambda) (None, 1) 0 softmax[0][0] the_labels[0][0] input_length[0][0] label_length[0][0] ================================================================================================== Total params: 7,423,594 Trainable params: 7,418,602 Non-trainable params: 4,992 __________________________________________________________________________________________________ . plot_model( model, to_file = &#39;model.png&#39;, show_shapes = True, show_layer_names = True, rankdir = &#39;TB&#39; ) . Training Model . model.compile(loss = {&#39;ctc&#39;: lambda y_true, y_pred: y_pred}, optimizer = Adadelta()) . WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead. . train_datagen = DataGenerator(&#39;./Train_Images&#39;, batch_size, img_w, img_h, downsample_factor, True, max_text_len) valid_datagen = DataGenerator(&#39;./Validation_Images&#39;, batch_size, img_w, img_h, downsample_factor, True, max_text_len) . early_stop = EarlyStopping( monitor = &#39;val_loss&#39;, min_delta = 0.001, patience = 4, mode = &#39;min&#39;, verbose = 1 ) checkpoint = ModelCheckpoint( filepath = &#39;CuDNNLSTM+BN5--{epoch:02d}--{loss:.3f}--{val_loss:.3f}.hdf5&#39;, monitor = &#39;val_loss&#39;, verbose = 1, mode = &#39;min&#39;, period = 1 ) . history = model.fit_generator( generator = train_datagen, steps_per_epoch = int(len(train_datagen.image_list) // batch_size), epochs = 30, callbacks = [checkpoint], validation_data = valid_datagen, validation_steps = int(len(valid_datagen.image_list) // batch_size) ) . Epoch 1/30 1171/1171 [==============================] - 812s 693ms/step - loss: 23.8972 - val_loss: 20.9087 Epoch 00001: saving model to CuDNNLSTM+BN5--01--23.897--20.909.hdf5 Epoch 2/30 1171/1171 [==============================] - 789s 674ms/step - loss: 21.5477 - val_loss: 22.0248 Epoch 00002: saving model to CuDNNLSTM+BN5--02--21.548--22.025.hdf5 Epoch 3/30 1171/1171 [==============================] - 790s 675ms/step - loss: 21.1628 - val_loss: 21.1927 Epoch 00003: saving model to CuDNNLSTM+BN5--03--21.163--21.193.hdf5 Epoch 4/30 1171/1171 [==============================] - 789s 673ms/step - loss: 21.3286 - val_loss: 20.7860 Epoch 00004: saving model to CuDNNLSTM+BN5--04--21.329--20.786.hdf5 Epoch 5/30 1171/1171 [==============================] - 797s 681ms/step - loss: 20.6932 - val_loss: 20.4295 Epoch 00005: saving model to CuDNNLSTM+BN5--05--20.693--20.430.hdf5 Epoch 6/30 1171/1171 [==============================] - 803s 686ms/step - loss: 19.6645 - val_loss: 18.8030 Epoch 00006: saving model to CuDNNLSTM+BN5--06--19.665--18.803.hdf5 Epoch 7/30 1171/1171 [==============================] - 802s 685ms/step - loss: 17.7595 - val_loss: 16.2503 Epoch 00007: saving model to CuDNNLSTM+BN5--07--17.760--16.250.hdf5 Epoch 8/30 1171/1171 [==============================] - 805s 688ms/step - loss: 13.9199 - val_loss: 15.0267 Epoch 00008: saving model to CuDNNLSTM+BN5--08--13.920--15.027.hdf5 Epoch 9/30 1171/1171 [==============================] - 805s 687ms/step - loss: 10.0150 - val_loss: 8.5883 Epoch 00009: saving model to CuDNNLSTM+BN5--09--10.015--8.588.hdf5 Epoch 10/30 1171/1171 [==============================] - 807s 689ms/step - loss: 7.3612 - val_loss: 7.0621 Epoch 00010: saving model to CuDNNLSTM+BN5--10--7.361--7.062.hdf5 Epoch 11/30 1171/1171 [==============================] - 806s 689ms/step - loss: 5.2345 - val_loss: 4.6761 Epoch 00011: saving model to CuDNNLSTM+BN5--11--5.235--4.676.hdf5 Epoch 12/30 1171/1171 [==============================] - 807s 689ms/step - loss: 3.8050 - val_loss: 3.4709 Epoch 00012: saving model to CuDNNLSTM+BN5--12--3.805--3.471.hdf5 Epoch 13/30 1171/1171 [==============================] - 807s 690ms/step - loss: 2.9636 - val_loss: 2.3625 Epoch 00013: saving model to CuDNNLSTM+BN5--13--2.964--2.363.hdf5 Epoch 14/30 1171/1171 [==============================] - 809s 691ms/step - loss: 2.3031 - val_loss: 2.2098 Epoch 00014: saving model to CuDNNLSTM+BN5--14--2.303--2.210.hdf5 Epoch 15/30 1171/1171 [==============================] - 808s 690ms/step - loss: 1.8695 - val_loss: 1.5765 Epoch 00015: saving model to CuDNNLSTM+BN5--15--1.869--1.576.hdf5 Epoch 16/30 1171/1171 [==============================] - 807s 689ms/step - loss: 1.4990 - val_loss: 1.2549 Epoch 00016: saving model to CuDNNLSTM+BN5--16--1.499--1.255.hdf5 Epoch 17/30 1171/1171 [==============================] - 808s 690ms/step - loss: 1.2347 - val_loss: 1.0244 Epoch 00017: saving model to CuDNNLSTM+BN5--17--1.235--1.024.hdf5 Epoch 18/30 1171/1171 [==============================] - 808s 690ms/step - loss: 0.9716 - val_loss: 1.1324 Epoch 00018: saving model to CuDNNLSTM+BN5--18--0.972--1.132.hdf5 Epoch 19/30 1171/1171 [==============================] - 808s 690ms/step - loss: 0.8176 - val_loss: 0.8872 Epoch 00019: saving model to CuDNNLSTM+BN5--19--0.818--0.887.hdf5 Epoch 20/30 1171/1171 [==============================] - 808s 690ms/step - loss: 0.7288 - val_loss: 0.7103 Epoch 00020: saving model to CuDNNLSTM+BN5--20--0.729--0.710.hdf5 Epoch 21/30 1171/1171 [==============================] - 807s 689ms/step - loss: 0.5723 - val_loss: 0.5983 Epoch 00021: saving model to CuDNNLSTM+BN5--21--0.572--0.598.hdf5 Epoch 22/30 1171/1171 [==============================] - 807s 689ms/step - loss: 0.4767 - val_loss: 0.5898 Epoch 00022: saving model to CuDNNLSTM+BN5--22--0.477--0.590.hdf5 Epoch 23/30 1171/1171 [==============================] - 803s 686ms/step - loss: 0.4160 - val_loss: 1.2394 Epoch 00023: saving model to CuDNNLSTM+BN5--23--0.416--1.239.hdf5 Epoch 24/30 1171/1171 [==============================] - 801s 684ms/step - loss: 0.3898 - val_loss: 0.4743 Epoch 00024: saving model to CuDNNLSTM+BN5--24--0.390--0.474.hdf5 Epoch 25/30 1171/1171 [==============================] - 804s 687ms/step - loss: 0.3087 - val_loss: 0.4375 Epoch 00025: saving model to CuDNNLSTM+BN5--25--0.309--0.437.hdf5 Epoch 26/30 1171/1171 [==============================] - 803s 686ms/step - loss: 0.3340 - val_loss: 0.3675 Epoch 00026: saving model to CuDNNLSTM+BN5--26--0.334--0.368.hdf5 Epoch 27/30 1171/1171 [==============================] - 802s 685ms/step - loss: 0.2315 - val_loss: 0.4050 Epoch 00027: saving model to CuDNNLSTM+BN5--27--0.232--0.405.hdf5 Epoch 28/30 1171/1171 [==============================] - 804s 687ms/step - loss: 0.2043 - val_loss: 0.3836 Epoch 00028: saving model to CuDNNLSTM+BN5--28--0.204--0.384.hdf5 Epoch 29/30 1171/1171 [==============================] - 806s 688ms/step - loss: 0.2105 - val_loss: 0.3188 Epoch 00029: saving model to CuDNNLSTM+BN5--29--0.211--0.319.hdf5 Epoch 30/30 1171/1171 [==============================] - 806s 688ms/step - loss: 0.1601 - val_loss: 0.3610 Epoch 00030: saving model to CuDNNLSTM+BN5--30--0.160--0.361.hdf5 . plt.plot(history.history[&#39;loss&#39;], color = &#39;b&#39;) plt.plot(history.history[&#39;val_loss&#39;], color = &#39;r&#39;) plt.show() . Region = {&quot;A&quot;: &quot;서울 &quot;, &quot;B&quot;: &quot;경기 &quot;, &quot;C&quot;: &quot;인천 &quot;, &quot;D&quot;: &quot;강원 &quot;, &quot;E&quot;: &quot;충남 &quot;, &quot;F&quot;: &quot;대전 &quot;, &quot;G&quot;: &quot;충북 &quot;, &quot;H&quot;: &quot;부산 &quot;, &quot;I&quot;: &quot;울산 &quot;, &quot;J&quot;: &quot;대구 &quot;, &quot;K&quot;: &quot;경북 &quot;, &quot;L&quot;: &quot;경남 &quot;, &quot;M&quot;: &quot;전남 &quot;, &quot;N&quot;: &quot;광주 &quot;, &quot;O&quot;: &quot;전북 &quot;, &quot;P&quot;: &quot;제주 &quot;} Hangul = {&quot;dk&quot;: &quot;아&quot;, &quot;dj&quot;: &quot;어&quot;, &quot;dh&quot;: &quot;오&quot;, &quot;dn&quot;: &quot;우&quot;, &quot;qk&quot;: &quot;바&quot;, &quot;qj&quot;: &quot;버&quot;, &quot;qh&quot;: &quot;보&quot;, &quot;qn&quot;: &quot;부&quot;, &quot;ek&quot;: &quot;다&quot;, &quot;ej&quot;: &quot;더&quot;, &quot;eh&quot;: &quot;도&quot;, &quot;en&quot;: &quot;두&quot;, &quot;rk&quot;: &quot;가&quot;, &quot;rj&quot;: &quot;거&quot;, &quot;rh&quot;: &quot;고&quot;, &quot;rn&quot;: &quot;구&quot;, &quot;wk&quot;: &quot;자&quot;, &quot;wj&quot;: &quot;저&quot;, &quot;wh&quot;: &quot;조&quot;, &quot;wn&quot;: &quot;주&quot;, &quot;ak&quot;: &quot;마&quot;, &quot;aj&quot;: &quot;머&quot;, &quot;ah&quot;: &quot;모&quot;, &quot;an&quot;: &quot;무&quot;, &quot;sk&quot;: &quot;나&quot;, &quot;sj&quot;: &quot;너&quot;, &quot;sh&quot;: &quot;노&quot;, &quot;sn&quot;: &quot;누&quot;, &quot;fk&quot;: &quot;라&quot;, &quot;fj&quot;: &quot;러&quot;, &quot;fh&quot;: &quot;로&quot;, &quot;fn&quot;: &quot;루&quot;, &quot;tk&quot;: &quot;사&quot;, &quot;tj&quot;: &quot;서&quot;, &quot;th&quot;: &quot;소&quot;, &quot;tn&quot;: &quot;수&quot;, &quot;gj&quot;: &quot;허&quot;} . def decode_label(out): # out : (1, 32, 42) out_best = list(np.argmax(out[0, 2:], axis=1)) # get max index -&gt; len = 32 out_best = [k for k, g in itertools.groupby(out_best)] # remove overlap value outstr = &#39;&#39; for i in out_best: if i &lt; len(letters): outstr += letters[i] return outstr . def label_to_hangul(label): # eng -&gt; hangul region = label[0] two_num = label[1:3] hangul = label[3:5] four_num = label[5:] try: region = Region[region] if region != &#39;Z&#39; else &#39;&#39; except: pass try: hangul = Hangul[hangul] except: pass return region + two_num + hangul + four_num . model = get_Model(training = False) model.load_weights(&#39;CuDNNLSTM+BN5--29--0.211--0.319.hdf5&#39;) . WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where . inp, out = valid_datagen.__getitem__(0) images = inp[&#39;the_input&#39;] images.shape . (128, 128, 64, 1) . predictions = model.predict(images[0].reshape(1, img_w, img_h, 1), verbose = 1) predictions.shape . 1/1 [==============================] - 2s 2s/step . (1, 32, 42) . pred_texts = decode_label(predictions) pred_texts . &#39;Z16th6435&#39; . plt.imshow(images[0].reshape(img_w, img_h).T) plt.title(pred_texts) . Text(0.5, 1.0, &#39;Z16th6435&#39;) . #collapse-hide fig, axes = plt.subplots(nrows = 4, ncols = 2, figsize = (16, 16)) plt.setp(axes.flat, xticks = [], yticks = []) c = 1 for i, ax in enumerate(axes.flat): image = inp[&#39;the_input&#39;][c].reshape(1, img_w, img_h, 1) prediction = model.predict(image) ax.imshow(image.reshape(img_w, img_h).T) ax.set_xlabel(decode_label(prediction) + &#39; n&#39; + &#39;Hangul: &#39; + label_to_hangul(decode_label(prediction))) c += 1 plt.show() . .",
            "url": "https://soumik12345.github.io/geekyrakshit-blog/computervision/deeplearning/anpr/numberplate/neuralnetwork/crnn/convolution/recurrent/keras/python/tensorflow/2020/04/13/anpr.html",
            "relUrl": "/computervision/deeplearning/anpr/numberplate/neuralnetwork/crnn/convolution/recurrent/keras/python/tensorflow/2020/04/13/anpr.html",
            "date": " • Apr 13, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Depthwise Separable Convolutions in Deep Learning",
            "content": "The Convolution operation is a widely used function in Functional Analysis, Image Processing Deep Learning. The convolution operation when applied on two functions f and g, produces a third function expressing how the shape of one is modified by the other. While it is immensely popular, especially in the domain of Deep Learning, the vanilla convolution operation is quite expensive computationally. Modern Neural Network architectures such as Xception and MobileNet use a special type of Convolution called Depthwise Separable Convolution to speed up training and inference, especially on Mobile and Embedded Devices. . The Vanilla Convolution Operation . The convolution function can be mathematically defined as the following: . (f⊛g)(t)=∫−∞∞f(τ)g(t−τ)dτ(f circledast g)(t) = int_{- infty}^{ infty} f( tau) g(t - tau) d tau(f⊛g)(t)=∫−∞∞​f(τ)g(t−τ)dτ . For all non-negative values of t (i.e, for all values of t such that t ∈ [0, ∞) ), we could truncate the limits of integration resulting in, . (f⊛g)(t)=∫0tf(τ)g(t−τ)dτ(f circledast g)(t) = int_{0}^{t} f( tau) g(t - tau) d tau(f⊛g)(t)=∫0t​f(τ)g(t−τ)dτ . It can also be defined as the overlap of two functions f and g as one slides over the other, performing a sum of products. . Source: https://en.wikipedia.org/wiki/Convolution Convolution as Sum of Products. Source: https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1 Computational Complexity of Convolution . In order to decide the computational complexity of the convolutional operation, we would count the number of multiplication operations for a convolution. This is because the Binary Addition of two numbers may be performed in a single clock cycle using two registers with the inputs latched and a bunch of combinatorial logic gates. Binary multiplication, however, requires successive shift and addition operations which must be performed as many times as there are bits in the multiplier and is thus a more expensive operation. . Let us consider an input volume of the dimension (DVD_{V}DV​, DVD_{V}DV​, N) where DVD_{V}DV​ is the height and width of the volume and NNN is the number of channels. In the case of a standard RGB image N=3N = 3N=3 and for a gray-scale image N=1N = 1N=1. . Let us convolve V with a tensor of shape (DVD_{V}DV​, DVD_{V}DV​, N) or N tensors with the shape (DkD_{k}Dk​, DkD_{k}Dk​) which results in a volume GGG of shape (DGD_{G}DG​, DGD_{G}DG​, N). . Let us count the number of multiplication operations for this operation. . Number of multiplication operations for a single stride across a single channel = Dk∗DkD_{k} * D_{k}Dk​∗Dk​. . For M channels in the initial volume, the number of multiplication operations = (Dk)2∗M(D_{k})^{2} * M(Dk​)2∗M. . Sliding the kernel over a volume of (DVD_{V}DV​, DVD_{V}DV​, M), we get a tensor of shape (DGD_{G}DG​, DGD_{G}DG​, N). Hence the total number of multiplication operations for a single channel of the convolution kernel = (DG)2∗(Dk)2∗M(D_{G})^{2} * (D_{k})^{2} * M(DG​)2∗(Dk​)2∗M. . Since there are N channels in the convolutional kernel, this operation is repeated N times. Hence, the total number of multiplication operations for the above convolution operation = N∗(DG)2∗(Dk)2∗MN * (D_{G})^{2} * (D_{k})^{2} * MN∗(DG​)2∗(Dk​)2∗M. . Now, let us see how using an alternate form of the vanilla convolution operation, we can reduce time complexity. . Depthwise Separable Convolution . In the vanilla convolution operation all, the kernel is applied to all the channels of the input volume. However, Depthwise Separable Convolutions breaks down the whole operation into 2 steps: . Depthwise Convolution or the Filtering Stage | Pointwise Convolution or the Combination Stage | Depthwise Convolutions . Let us consider the same input volume (DVD_{V}DV​, DVD_{V}DV​, M) convolving with M (DKD_{K}DK​, DKD_{K}DK​) kernels. A single convolution with a single kernel gives a volume of (DGD_{G}DG​, DGD_{G}DG​, 1). Repeating this N times, we get N such tensors and stacking them up channel-wise, we get a single tensor of shape (DGD_{G}DG​, DGD_{G}DG​, M). . Now, the number of multiplication operations for a single kernel convolving over a single input channel = DK∗DKD_{K} * D_{K}DK​∗DK​. When the convolution is applied over an entire input volume . Let us now find the computational complexity for Depthwise Convolution. . The number of multiplication operations for the convolution of a single (DKD_{K}DK​, DKD_{K}DK​) kernel over a single stride over the input volume = (DK)2(D_{K})^{2}(DK​)2. . Since the output shape is (DGD_{G}DG​, DGD_{G}DG​), the number of multiplication operations for convolving over a single channel of the input image = (DG)2∗(DK)2(D_{G})^{2} * (D_{K})^{2}(DG​)2∗(DK​)2. . Since there are MMM number of kernels for convolving with MMM number of channels, the number of multiplication operations for Depthwise Convolution operation = M∗(DG)2∗(DK)2M * (D_{G})^{2} * (D_{K})^{2}M∗(DG​)2∗(DK​)2. . Pointwise Convolution . For Pointwise Convolution, we convolve the (DGD_{G}DG​, DGD_{G}DG​, M) volume with NNN kernels of (1, 1, MMM) producing the desired output of shape (DVD_{V}DV​, DVD_{V}DV​, N). . We will now find the computational complexity of the Pointwise Convolution operation. . For convolving a single kernel over a single stride of the input image, the number of multiplication operations = 1∗1∗M1 * 1 * M1∗1∗M = MMM. . For convolving a single kernel over a single channel of the input tensor producing a shape of (DGD_{G}DG​, DGD_{G}DG​), the number of multiplication operations = M∗(DG)2M * (D_{G})^{2}M∗(DG​)2. . For convolving NNN number of kernels over the whole of input tensor, the number of multiplication operations = N∗M∗(DG)2N * M * (D_{G})^{2}N∗M∗(DG​)2. . Comparing Vanilla Convolution with Depthwise Separable Convolution . Let us take the ratios between the Complexity of the Vanilla Convolution () operation and that of the Depthwise Separable Convolution operation. . convvanillaconvdsc=N∗(DG)2∗(DK)2∗MM∗(DG)2∗[(DK)2+N] frac{conv_{vanilla}}{conv_{dsc}} = frac{N * (D_{G})^{2} * (D_{K})^{2} * M}{M * (D_{G})^{2} * [(D_{K})^{2} + N]}convdsc​convvanilla​​=M∗(DG​)2∗[(DK​)2+N]N∗(DG​)2∗(DK​)2∗M​ . or, . convvanillaconvdsc=(DK)2∗M(DK)2+N frac{conv_{vanilla}}{conv_{dsc}} = frac{(D_{K})^{2} * M}{(D_{K})^{2} + N}convdsc​convvanilla​​=(DK​)2+N(DK​)2∗M​ . or, . convvanillaconvdsc=1(DK)2+1N frac{conv_{vanilla}}{conv_{dsc}} = frac{1}{(D_{K})^{2}} + frac{1}{N}convdsc​convvanilla​​=(DK​)21​+N1​ . Now, let us consider N=3N = 3N=3 and DK = [2 ** i for i in range(5, 11)] and visualize how the ratio varies. . Note that the ratio of Time Complexity of Vanilla Convolution to that of Depthwise Separable Convolution is always much less than 1 and it decreases with increasing Kernel Dimension, making it much faster compared to Vanilla Convolution. . Depthwise Separable Convolutions are widely used in building fast CNN architectures such as Xception, Mobilenet and Multi-modal Neural Networks. In the upcoming articles, we would discuss these two articles in detail. .",
            "url": "https://soumik12345.github.io/geekyrakshit-blog/algebra/cnn/computervision/convolution/convolutionaneuralnetwork/datascience/deeplearning/depthwiseseperableconvolution/dnn/embeddeddevices/google/mathematics/maths/mobiledevices/mobilenet/multimodealnetwork/neural-networks/plotly/python/xception/2019/10/19/depthwise-seperable-convolution.html",
            "relUrl": "/algebra/cnn/computervision/convolution/convolutionaneuralnetwork/datascience/deeplearning/depthwiseseperableconvolution/dnn/embeddeddevices/google/mathematics/maths/mobiledevices/mobilenet/multimodealnetwork/neural-networks/plotly/python/xception/2019/10/19/depthwise-seperable-convolution.html",
            "date": " • Oct 19, 2019"
        }
        
    
  
    
        ,"post3": {
            "title": "Nearest Celebrity Face using Deep Learning",
            "content": "Face Recognition . Face Recognition is one of the more interesting applications of Deep Learning i. At a single glance, it may seem like a simple classification problem; classify if this photo shows Soumik’s face or Souranil’s. You might be quick to jump to the conclusion, oh, the handsome face on the right belongs to Soumik and the ugly face on the left belongs to Souranil but, Ahem!!! We beg to differ. . Dear ConvNet, which of these is Soumik and which one is Souranil??? See, the problem with traditional ConvNets is that they need to look at lots of images of your face and lots of other faces to learn to correctly classify them; maybe a thousand images from each category. Imagine yourself as the teacher of a class where you want to install a system, which monitors every student and recognize them by faces so that they would not bunk classes (poor students 😓). In order to do so you have to collect 1000 mugshots of each of your students!!! Even if you manage to do this insane task, just imagine having to retrain the model again if a new student decides to join your lecture!!! . Ideally, we would want to verify the face of a person from any footage given only one photo of the person available in the database. Hence, our challenge, in this case, can be formalized as a One-Shot Learning problem. History has been witness to the fact that Deep Learning algorithms do not work well if you have only one training example. . One-Shot Learning . Let’s focus a bit more light on the problem discussed earlier with an example. Our dear friend Atul is supposed to appear for a video interview for a company and he wants to remain undisturbed during the interview, so he won’t allow anyone into his room other than Abhik who speaks a lot of Crox English and would help him with the interview (from behind the laptop of course). So, he wants sets up a system using his phone camera which would verify if the person who wants to enter his room is Abhik or not. Now, Atul can simply design a ConvNet with convolutional layers followed by a couple of fully connected layers ending in a softmax activation with 3 outputs corresponding to the three of us. . Which of us speaks the best Crox will be decided upon by Atul’s algorithm!!! There are several demerits to this approach such as . Atul does not have more than thirty of our mugshots which he painfully collected from Facebook and Instagram. Such a small training set would not be enough. | If he decides to accept help from Avishek besides Abhik, he would have to retrain the model again (after painfully collecting his mugshots) in order for the algorithm to recognize Avishek also. | . So, in order to make this work, Atul thinks of a different approach. Instead of building a model to differentiate various faces, he decides to build a model that would learn a Similarity Function D. Then that would say how similar the current image is with a mugshot of Abhik that is present in his dataset and he decides upon a similarity threshold τ upon meeting which the door will open. . D(image1, image2) = Degree of Difference between the Images D(image1, image2) &lt;= τ means images are same, and D(image1, image2) &gt; τ means images are different Siamese Networks . The job of the function D is to learn the level of difference between two different images of faces. A nice way to do this is by using Siamese Networks. While a traditional ConvNet consists of Convolutional layers followed by Fully Connected or Dense Layers which are then fed into a Softmax function to perform classification, in case of a Siamese Network, there is no Softmax unit, instead the last Dense layer acts as the output layers which gives a list or vector of numbers. Let us call the outputs for images x_1 and x_2, f(x_1) and f(x_2) respectively. Let’s say that the output layer has 128 fully connected units, hence, f(x_1) and f(x_2) will each be a vector of 128 numbers. f(x_1) and f(x_2) are called the Encoding of x_1 and x_2 respectively. . If we believe the encodings to be a good enough representation of the input images, we can define the function D as the square of norm of the difference between the Encodings. . D(x1,x2)=∣∣f(x1)−f(x2)∣∣2D(x_{1}, x_{2}) = ||f(x_{1}) - f(x_{2})||^{2}D(x1​,x2​)=∣∣f(x1​)−f(x2​)∣∣2 . Now the question comes that how do we train such a network. Since the same network is used to compute the Encodings from two different images, we have to train the parameters so that the trained network defines as accurate Encoding. . Training a Siamese Network . One way to learn the parameters of the neural network so that it gives you an accurate enough encoding for the images is to define an applied gradient descent on the Triplet Loss Function. In this case, we will have an anchor image, a positive image (the same person as the anchor image) and a negative image (a different person from the anchor image). Now we train the Siamese Network so that the distance between the anchor and the positive image is minimized. This also increases the distance between the anchor and the negative image is maximized. The fact that at each instance, we are looking at three images gives rise to the terminology Triplet Loss. The dataset, in this case, should consist of multiple triplets of Anchor, Positive and Negative Images. . Learning Objective . For Positive Image, . D(A,P)=∣∣f(A)−f(P)∣∣2D(A, P) = ||f(A) - f(P)||^{2}D(A,P)=∣∣f(A)−f(P)∣∣2 . For Negative Image, . D(A,N)=∣∣f(A)−f(N)∣∣2D(A, N) = ||f(A) - f(N)||^{2}D(A,N)=∣∣f(A)−f(N)∣∣2 . Learning Objective is . D(A,P)−D(A,N)+α≤0D(A, P) - D(A, N) + alpha leq 0D(A,P)−D(A,N)+α≤0 . where α is defined as a margin and the Loss Function is given as . loss=∑i=1nmax(∣∣f(Ai)−f(Pi)∣∣2−∣∣f(Ai)−f(Pi)∣∣2+α,0)loss = sum_{i=1}^{n} max(||f(A_{i}) - f(P_{i})||^{2} - ||f(A_{i}) - f(P_{i})||^{2} + alpha, 0)loss=i=1∑n​max(∣∣f(Ai​)−f(Pi​)∣∣2−∣∣f(Ai​)−f(Pi​)∣∣2+α,0) . where n is the number of triplets in the dataset. . All the ideas till this point have been presented in the paper FaceNet by Florian Schroff, Dmitry Kalinichenko, and James Philbin. . After having trained the FaceNet on a large Triplet Dataset, we can use it to verify any face. Now, Atul would only need to store the Encodings of the faces of Abhik and Avishek. Then he would have to decide upon the value of Similarity Threshold τ. Whenever someone approaches the door, his face will be localized and passed through the FaceNet. If their D value is less than τ then it is a match!!! . Extending the Idea to Nearest Celebrity Face . I always wondered about those Facebook applications which used to predict stuff like which Celebrity or Footballer you look like. I tried building a similar application using the idea of FaceNet. Instead of keeping images of existing people to be verified in the database, I collected images of famous celebrities from Google Images, closely cropped their faces and stored their Encodings in the database. The Encodings were generated by a pre-trained FaceNet with an Inception backbone. Now theoretically, if I input some of my friends’ faces, they should ideally match the closest image in the database. Well, I tried implementing it and the results were hilarious 😂 . The project can be found at https://github.com/soumik12345/Nearest-Celebrity-Face. If you like the project and found the results to be hilarious, please leave a star on the Github repository. For more such exciting articles, stay tuned at http://geekyrakshit.com. .",
            "url": "https://soumik12345.github.io/geekyrakshit-blog/computervision/deeplearning/facenet/inception/keras/nearestcelebrityface/python/tensorflow/2019/08/07/nearest-celebrity-face.html",
            "relUrl": "/computervision/deeplearning/facenet/inception/keras/nearestcelebrityface/python/tensorflow/2019/08/07/nearest-celebrity-face.html",
            "date": " • Aug 7, 2019"
        }
        
    
  

  
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://soumik12345.github.io/geekyrakshit-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}